{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp fastai.learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner Errors\n",
    "> In-place fastai specific errors to ease debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastdebug.torch import layer_error, device_error\n",
    "\n",
    "from fastai.data.all import *\n",
    "from fastai.optimizer import *\n",
    "from fastai.learner import *\n",
    "from fastai.callback.core import event\n",
    "from fastai.callback.training import ShortEpochCallback\n",
    "from fastai.torch_core import default_device\n",
    "\n",
    "\n",
    "from fastcore.basics import patch\n",
    "from fastcore.meta import delegates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a series of various errors that can be used when running with `fastai`. It should be noted here that there is no other imports or magic you need to do to use this section of the library other then: `from fastdebug import *`. It will automatically load in what's needed.\n",
    "\n",
    "As a style choice, we are choosing to do the `.*` notation as this loads in not only all of our errors, but also replaces sections of `fastai`'s code to inject some error handling (as we'll see later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def loss_func_error(e:Exception, learn) -> Exception:\n",
    "    \"\"\"\n",
    "    Error that should be run when there is an issue when working with the loss function\n",
    "    \n",
    "    Raises with a message stating the shapes of the inputs and targs, and the error\n",
    "    \"\"\"\n",
    "    err = f'There was an issue with calculating the loss with `{getattr(learn.loss_func, \"__name__\", learn.loss_func)}`'\n",
    "    err += f'\\n\\nPrediction shape(s): {[p.shape for p in listify(learn.pred)]}'\n",
    "    err += f'\\nLabel Shape(s): {[y.shape for y in learn.yb]}'\n",
    "    err += f'\\nError: {e.args[0]}'\n",
    "    e.args = [err]\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def callback_error(e:Exception, cb:str, event_name:str) -> Exception:\n",
    "    \"\"\"\n",
    "    Raises an error from when a Callback event failed, showing what event, the name of the Callback and the trace\n",
    "    \"\"\"\n",
    "    e.args = [f\"Exception raised in the {cb} Callback during {event_name}:\\n\\n{e.args[0]}\"]\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def catch_pred_errors(e:Exception, model) -> Exception:\n",
    "    \"Catches any errors relating to prediction that are either related to the device or model layers. Else raise `e`\"\n",
    "    if \"Input type\" in e.args[0]: device_error(e, 'Input', 'Model weights')\n",
    "    elif \"Expected\" in e.args[0]: layer_error(e, model)\n",
    "    else: raise e # anything else "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def catch_loss_errors(e:Exception, learn):\n",
    "    \"Catches any errors that occur with the loss function and its calculation\"\n",
    "    if \"Input type\" in e.args[0]: device_error(e, 'Model prediction', 'Truths')\n",
    "    else: loss_func_error(e, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifications and Enhancements to the fastai Source Code and `Learner`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def sanity_check(self:Learner, show_table=False):\n",
    "    \"Performs a short epoch and uses all the callbacks in `self.cbs` on the CPU to ensure nothing is broken\"\n",
    "    device = getattr(self.dls, 'device', default_device())\n",
    "    if hasattr(self.dls, 'device'):\n",
    "        self.dls.device = 'cpu'\n",
    "    else:\n",
    "        # Using raw torch\n",
    "        self.model.to('cpu')\n",
    "    self.save('tmp')\n",
    "    cbs = [ShortEpochCallback(short_valid=False)]\n",
    "    if show_table:\n",
    "        with self.no_bar(), self.no_logging():\n",
    "            self.fit(1, cbs=cbs)\n",
    "    else:\n",
    "        self.fit(1, cbs=cbs)\n",
    "    if hasattr(self.dls, 'device'):\n",
    "        self.dls.device = device\n",
    "    else:\n",
    "        self.model.to(device)\n",
    "    self.load('tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "@delegates(Learner.sanity_check)\n",
    "def __init__(self:Learner, dls, model, loss_func=None, opt_func=Adam, lr=defaults.lr, splitter=trainable_params, cbs=None,\n",
    "                 metrics=None, path=None, model_dir='models', wd=None, wd_bn_bias=False, train_bn=True,\n",
    "                 moms=(0.95,0.85,0.95), sanity_check=False, **kwargs):\n",
    "    \"Group together a `model`, some `dls` and a `loss_func` to handle training, potentially run a sanity check\"\n",
    "    path = Path(path) if path is not None else getattr(dls, 'path', Path('.'))\n",
    "    if loss_func is None:\n",
    "        loss_func = getattr(dls.train_ds, 'loss_func', None)\n",
    "        assert loss_func is not None, \"Could not infer loss function from the data, please pass a loss function.\"\n",
    "    self.dls,self.model = dls,model\n",
    "    store_attr(but='dls,model,cbs')\n",
    "    self.training,self.create_mbar,self.logger,self.opt,self.cbs = False,True,print,None,L()\n",
    "    self.add_cbs(L(defaults.callbacks)+L(cbs))\n",
    "    self(\"after_create\")\n",
    "    if sanity_check: self.sanity_check(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.__init__\" class=\"doc_header\"><code>Learner.__init__</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.__init__</code>(**`dls`**, **`model`**, **`loss_func`**=*`None`*, **`opt_func`**=*`Adam`*, **`lr`**=*`0.001`*, **`splitter`**=*`trainable_params`*, **`cbs`**=*`None`*, **`metrics`**=*`None`*, **`path`**=*`None`*, **`model_dir`**=*`'models'`*, **`wd`**=*`None`*, **`wd_bn_bias`**=*`False`*, **`train_bn`**=*`True`*, **`moms`**=*`(0.95, 0.85, 0.95)`*, **`sanity_check`**=*`False`*, **`show_table`**=*`False`*)\n",
       "\n",
       "Group together a `model`, some `dls` and a `loss_func` to handle training, potentially run a sanity check"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.sanity_check\" class=\"doc_header\"><code>Learner.sanity_check</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.sanity_check</code>(**`show_table`**=*`False`*)\n",
       "\n",
       "Performs a short epoch and uses all the callbacks in `self.cbs` on the CPU to ensure nothing is broken"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.sanity_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `sanity_check`, you can make sure that you've set everything up properly and you won't get any issues before pushing to the GPU. This allows you to quickly ensure that you won't get any `CUDA` device-assist errors, and that the whole training regiment will go well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def _do_one_batch(self:Learner):\n",
    "    try:\n",
    "        self.pred = self.model(*self.xb)\n",
    "    except RuntimeError as e:\n",
    "        catch_pred_errors(e, self.model)\n",
    "    self('after_pred')\n",
    "    if len(self.yb):\n",
    "        try:\n",
    "            self.loss_grad = self.loss_func(self.pred, *self.yb)\n",
    "        except Exception as e:\n",
    "            catch_loss_errors(e, self)\n",
    "        self.loss = self.loss_grad.clone()\n",
    "    self('after_loss')\n",
    "    if not self.training or not len(self.yb): return\n",
    "    self('before_backward')\n",
    "    self.loss_grad.backward()\n",
    "    self._with_events(self.opt.step, 'step', CancelStepException)\n",
    "    self.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def _call_one(self:Learner, event_name):\n",
    "    if not hasattr(event, event_name): raise Exception(f'missing {event_name}')\n",
    "    for cb in self.cbs.sorted('order'):\n",
    "        try:\n",
    "            cb(event_name)\n",
    "        except Exception as e:\n",
    "            callback_error(e, cb.__repr__(), event_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def module_error(e:AttributeError) -> AttributeError:\n",
    "    \"\"\"\n",
    "    Raises an error when trying to load in a previous `Learner` and custom functions were not available in the namespace\n",
    "    \"\"\"\n",
    "    args = e.args[0]\n",
    "    err = 'Custom classes or functions exported with your `Learner` are not available in the namespace currently.\\n'\n",
    "    err += 'Please re-declare them before calling `load_learner`:\\n'\n",
    "    err += args\n",
    "    e.args = [err]\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_learner(fname, cpu=True, pickle_module=pickle):\n",
    "    \"Load a `Learner` object in `fname`, optionally putting it on the `cpu`\"\n",
    "    distrib_barrier()\n",
    "    try: res = torch.load(fname, map_location='cpu' if cpu else None, pickle_module=pickle_module)\n",
    "    except AttributeError as e: module_error(e)\n",
    "    if hasattr(res, 'to_fp32'): res = res.to_fp32()\n",
    "    if cpu: res.dls.cpu()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a custom `load_learner` function here that can check if everything exported is available when bringing the model in, if not then it'll raise an explicit error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
