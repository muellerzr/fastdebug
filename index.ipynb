{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastdebug.torch import *\n",
    "from fastdebug.fastai import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fastdebug\n",
    "\n",
    "> A helpful library for improving torch and fastai errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install fastdebug`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fastdebug` is designed around improving the quality of life when dealing with Pytorch and fastai errors, while also including some new sanity checks (fastai only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch\n",
    "\n",
    "Pytorch now has:\n",
    "* `device_error`\n",
    "* `layer_error`\n",
    "\n",
    "Both can be imported with:\n",
    "```python\n",
    "from fastdebug.error.torch import device_error, layer_error\n",
    "```\n",
    "\n",
    "`device_error` prints out a much more readable error for when two tensors aren't on the same device:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "inp = torch.rand().cuda()\n",
    "model = model.cpu()\n",
    "try:\n",
    "    _ = model(inp)\n",
    "except Exception as e:\n",
    "    device_error(e, 'Input type', 'Model weights')\n",
    "```\n",
    "And our new log:\n",
    "```bash\n",
    "---------------------------------------------------------------------------\n",
    "RuntimeError                              Traceback (most recent call last)\n",
    "<ipython-input-28-981e0ace9c38> in <module>()\n",
    "      2     model(x)\n",
    "      3 except Exception as e:\n",
    "----> 4     device_error(e, 'Input type', 'Model weights')\n",
    "\n",
    "10 frames\n",
    "/usr/local/lib/python3.7/dist-packages/torch/tensor.py in __torch_function__(cls, func, types, args, kwargs)\n",
    "    993 \n",
    "    994         with _C.DisableTorchFunction():\n",
    "--> 995             ret = func(*args, **kwargs)\n",
    "    996             return _convert(ret, cls)\n",
    "    997 \n",
    "\n",
    "RuntimeError: Mismatch between weight types\n",
    "\n",
    "Input type has type: \t\t (torch.cuda.FloatTensor)\n",
    "Model weights have type: \t (torch.FloatTensor)\n",
    "\n",
    "Both should be the same.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with `layer_error`, if there is a shape mismatch it will attempt to find the right layer it was at:\n",
    "```python\n",
    "inp = torch.rand(5,2, 3)\n",
    "try:\n",
    "    m(inp)\n",
    "except Exception as e:\n",
    "    layer_error(e, m)\n",
    "```\n",
    "\n",
    "```python\n",
    "---------------------------------------------------------------------------\n",
    "RuntimeError                              Traceback (most recent call last)\n",
    "<ipython-input-84-d4ab91131841> in <module>()\n",
    "      3     m(inp)\n",
    "      4 except Exception as e:\n",
    "----> 5     layer_error(e, m)\n",
    "\n",
    "<ipython-input-83-ca2dc02cfff4> in layer_error(e, model)\n",
    "      8     i, layer = get_layer_by_shape(model, shape)\n",
    "      9     e.args = [f'Size mismatch between input tensors and what the model expects\\n\\n{args}\\n\\tat layer {i}: {layer}']\n",
    "---> 10     raise e\n",
    "\n",
    "<ipython-input-84-d4ab91131841> in <module>()\n",
    "      1 inp = torch.rand(5,2, 3)\n",
    "      2 try:\n",
    "----> 3     m(inp)\n",
    "      4 except Exception as e:\n",
    "      5     layer_error(e, m)\n",
    "\n",
    "/mnt/d/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n",
    "    725             result = self._slow_forward(*input, **kwargs)\n",
    "    726         else:\n",
    "--> 727             result = self.forward(*input, **kwargs)\n",
    "    728         for hook in itertools.chain(\n",
    "    729                 _global_forward_hooks.values(),\n",
    "\n",
    "/mnt/d/lib/python3.7/site-packages/torch/nn/modules/container.py in forward(self, input)\n",
    "    115     def forward(self, input):\n",
    "    116         for module in self:\n",
    "--> 117             input = module(input)\n",
    "    118         return input\n",
    "    119 \n",
    "\n",
    "/mnt/d/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n",
    "    725             result = self._slow_forward(*input, **kwargs)\n",
    "    726         else:\n",
    "--> 727             result = self.forward(*input, **kwargs)\n",
    "    728         for hook in itertools.chain(\n",
    "    729                 _global_forward_hooks.values(),\n",
    "\n",
    "/mnt/d/lib/python3.7/site-packages/torch/nn/modules/conv.py in forward(self, input)\n",
    "    421 \n",
    "    422     def forward(self, input: Tensor) -> Tensor:\n",
    "--> 423         return self._conv_forward(input, self.weight)\n",
    "    424 \n",
    "    425 class Conv3d(_ConvNd):\n",
    "\n",
    "/mnt/d/lib/python3.7/site-packages/torch/nn/modules/conv.py in _conv_forward(self, input, weight)\n",
    "    418                             _pair(0), self.dilation, self.groups)\n",
    "    419         return F.conv2d(input, weight, self.bias, self.stride,\n",
    "--> 420                         self.padding, self.dilation, self.groups)\n",
    "    421 \n",
    "    422     def forward(self, input: Tensor) -> Tensor:\n",
    "\n",
    "RuntimeError: Size mismatch between input tensors and what the model expects\n",
    "\n",
    "Model expected 4-dimensional input for 4-dimensional weight [3, 3, 1, 1], but got 3-dimensional input of size [5, 2, 3] instead\n",
    "\tat layer 1: Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fastai\n",
    "\n",
    "Along with the additions above (and are used during `fit`), fastai now has a `Learner.sanity_check` function, which allows you to quickly perform a basic check to ensure that your call to `fit` won't raise any exceptions. They are performed on the CPU for a partial epoch to make sure that `CUDA` device-assist errors can be preemptively found.\n",
    "\n",
    "To use it simply do:\n",
    "```python\n",
    "from fastdebug.fastai import *\n",
    "from fastai.vision.all import *\n",
    "\n",
    "learn = Learner(...)\n",
    "learn.sanity_check()\n",
    "```\n",
    "\n",
    "This is also now an argument in `Learner`, set to `False` by default, so that after making your `Learner` a quick check is ensured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "learn = Learner(..., sanity_check=True)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
